#### diabetes-eda-python
# Machine Learning Practice ‚Äî Diabetes Dataset (Google Colab, Python)
![Status](https://img.shields.io/badge/status-completed-brightgreen)
![Type](https://img.shields.io/badge/project-learning-blue)
![ML](https://img.shields.io/badge/topic-machine%20learning-lightgrey)
![Python](https://img.shields.io/badge/language-python-blue)
![Colab](https://img.shields.io/badge/notebook-colab-orange)

> This project was completed as part of a personal assignment to practice data analysis and model building using Python.  
> The dataset is the **Diabetes dataset from `sklearn.datasets`** containing anonymized medical features and a continuous target related to disease progression.  
> The goal of the assignment was **not clinical research**, but rather to:  
> ‚úÖ apply Python for data exploration and EDA  
> ‚úÖ build and compare basic regression machine learning models  
> ‚úÖ evaluate and interpret results  

> For educational use only ‚Äî do not redistribute commercially

---

### Objective
- Use the **sklearn diabetes dataset** to practice the full machine learning workflow, from data exploration ‚Üí visualization ‚Üí regression model building and comparison (**Linear Regression, Decision Tree Regressor, Random Forest Regressor**).

---

### Tools & Tech:
| Tool              | Purpose                            |
|-------------------|-------------------------------------|
| `Python`            | Programming language                |
| `Pandas`            | Data cleaning/manipulation          |
| `Matplotlib` / `Seaborn` | Visualization / EDA              |
| `Scikit-Learn`      | ML models (Linear Regression, Decision Tree Regressor, Random Forest Regressor) |
| `Google Colab`      | Notebook environment                |

---

### Workflow Summary
1. **Pre-processing**
   - Load dataset
   - Confirm no missing values
   - Check feature types and scaling
2. **Exploratory Data Analysis (EDA)**
   - Summary statistics
   - Heatmap of feature correlations
   - Distribution plots
3. **Model Training & Evaluation**
   - Train/test split  
   - Linear Regression model  
   - Decision Tree Regressor  
   - Random Forest Regressor  
   - Compared models using **R¬≤ score and Mean Squared Error (MSE)**
4. **Conclusion**
   - Identified which variables have strong impact on prediction
   - Compared model performance
   - Reflected on improvements / next steps

---

### Key Takeaways (from this exercise)
- **Linear Regression delivered the best performance** among the three tested models (R¬≤ = **0.45**, mean cross-validated R¬≤ ‚âà **0.48 ¬± 0.05**).  
- **Decision Tree Regressor** and **Random Forest Regressor** did not provide significant improvement over the linear model ‚Äî suggesting that the relationship between input features and the target variable in this dataset is mostly linear.  
- **Cross-validation** confirms that **Linear Regression** is the most stable model (lowest standard deviation of R¬≤), while Decision Tree showed the most variability.  
- The dataset contained **no missing values** and was **standardized** prior to modeling.  
- **bmi** and **s5 (a serum measurement)** showed the **strongest positive correlation** with the target (0.59 and 0.57 respectively).  
- Random Forest feature importance also identified **bmi** and **s5** as the most influential predictors.  
- Overall, the dataset has **moderate predictive capability** ‚Äî visualization of actual vs. predicted values shows that even the best-performing model still has room for improvement.

---

### ‡∏™‡∏£‡∏∏‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
- ‡πÇ‡∏°‡πÄ‡∏î‡∏• **Linear Regression** ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏î‡∏≤ 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏à‡∏≥‡∏•‡∏≠‡∏á (R¬≤ = **0.45** ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ Cross-Validation ‚âà **0.48 ¬± 0.05**)  
- **Decision Tree Regressor** ‡πÅ‡∏•‡∏∞ **Random Forest Regressor** ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏ã‡∏∂‡πà‡∏á‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á feature ‡∏Å‡∏±‡∏ö target ‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô (linear) ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å  
- ‡∏à‡∏≤‡∏Å‡∏ú‡∏• **Cross-Validation** ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ **Linear Regression** ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà Decision Tree ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤  
- Dataset ‡∏ô‡∏µ‡πâ **‡πÑ‡∏°‡πà‡∏°‡∏µ missing value** ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô (standardize) ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ train ‡πÇ‡∏°‡πÄ‡∏î‡∏•  
- **‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö target ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î** ‡∏Ñ‡∏∑‡∏≠ **bmi (‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏°‡∏ß‡∏•‡∏Å‡∏≤‡∏¢)** ‡πÅ‡∏•‡∏∞ **s5 (‡∏£‡∏∞‡∏î‡∏±‡∏ö serum ‡∏ó‡∏µ‡πà 5)** (‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå 0.59 ‡πÅ‡∏•‡∏∞ 0.57 ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö)  
- ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå feature importance ‡∏Ç‡∏≠‡∏á Random Forest ‡∏Å‡πá‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ **bmi** ‡πÅ‡∏•‡∏∞ **s5** ‡πÄ‡∏õ‡πá‡∏ô feature ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¥‡∏ó‡∏ò‡∏¥‡∏û‡∏•‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô  
- ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß dataset ‡∏ô‡∏µ‡πâ‡∏°‡∏µ **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á** ‚Äì ‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ Linear Regression ‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö actual vs predicted ‡∏¢‡∏±‡∏á‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡πÑ‡∏õ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï  

---

### Notebook
#### View the full code and analysis in Google Colab üëâ [Open Notebook](https://colab.research.google.com/drive/1-2G1mkhT9AJHWs9Kz0MCbKWa3AoTbEN6?usp=sharing)

---

### Possible Improvements (next iteration)
- Hyperparameter tuning (GridSearchCV / RandomizedSearchCV)  
- Add **Support Vector Regressor (SVR)** for comparison  
- Implement **K-Fold cross validation** (instead of simple train-test split)  
- Build a simple **Streamlit app** to input features and predict progression score  

---
