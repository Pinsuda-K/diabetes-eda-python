### diabetes-eda-python
# Machine Learning Practice ‚Äî Diabetes Dataset (Google Colab, Python)
![Status](https://img.shields.io/badge/status-completed-brightgreen)
![Type](https://img.shields.io/badge/project-learning-blue)
![ML](https://img.shields.io/badge/topic-machine%20learning-lightgrey)
![Python](https://img.shields.io/badge/language-python-blue)
![Colab](https://img.shields.io/badge/notebook-colab-orange)

> This project was completed as part of a personal assignment to practice data analysis and model building using Python. The dataset contains anonymized medical information related to diabetes. The goal of the assignment was **not clinical research**, but rather to:
> ‚úÖ apply Python for data cleaning and EDA
> ‚úÖ build and compare basic machine learning models
> ‚úÖ evaluate and interpret results

### Objective
- Use a diabetes dataset to practice the full machine learning workflow, from data cleaning ‚Üí visualization ‚Üí model building and comparison (Logistic Regression, Decision Tree, etc.).
---
### Tools & Tech:
| Tool              | Purpose                            |
|-------------------|-------------------------------------|
| `Python`            | Programming language                |
| `Pandas`            | Data cleaning/manipulation          |
| `Matplotlib` / `Seaborn` | Visualization / EDA              |
| `Scikit-Learn`      | ML models (LogReg, DecisionTree, ...) |
| `Google Colab`      | Notebook environment                |
---
### Workflow Summary
1. **Pre-processing**
   - Load dataset
   - Check and handle missing values
   - Convert data types / scale features (if needed)
2. **Exploratory Data Analysis (EDA)**
   - Summary statistics
   - Heatmap of feature correlations
   - Distribution plots
3. **Model Training & Evaluation**
   - Train/test split  
   - Logistic Regression model  
   - Decision Tree model  
   - Compared models using accuracy + confusion matrix
4. **Conclusion**
   - Identified which variables have strong impact on prediction
   - Compared model performance
   - Reflected on improvements / next steps
---
### Key Takeaways (from this exercise)
- **Linear Regression delivered the best performance** among the three tested models (R¬≤ = **0.45**, mean cross-validated R¬≤ ‚âà **0.48 ¬± 0.05**).
- **Decision Tree** and **Random Forest** did not provide significant improvement over the linear model ‚Äî suggesting that the relationship between input features and the target variable in this dataset is mostly linear.
- **Cross-validation** confirms that **Linear Regression** is the most stable model (lowest standard deviation of R¬≤), while Decision Tree produced the most variability.
- The dataset contained **no missing values** and was **standardized** prior to modeling.
- bmi and s5 (a serum measurement) showed the **strongest positive correlation** with the target (0.59 and 0.57 respectively).
- Random Forest feature importance also identified bmi and s5 as the most influential predictors.
- Overall, the dataset has **moderate predictive capability** ‚Äî visualization of actual vs. predicted values shows that even the best-performing model still has room for improvement.

### ‡∏™‡∏£‡∏∏‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
- ‡πÇ‡∏°‡πÄ‡∏î‡∏• **Linear Regression** ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏î‡∏≤ 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö (R¬≤ = **0.45** ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ Cross-Validation ‚âà **0.48 ¬± 0.05**)
- **Decision Tree** ‡πÅ‡∏•‡∏∞ **Random Forest** ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏ã‡∏∂‡πà‡∏á‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á feature ‡∏Å‡∏±‡∏ö target ‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô (linear) ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
- ‡∏à‡∏≤‡∏Å‡∏ú‡∏• **Cross-Validation** ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ **Linear Regression** ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà Decision Tree ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤
- Dataset ‡∏ô‡∏µ‡πâ **‡πÑ‡∏°‡πà‡∏°‡∏µ missing value** ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô (standardize) ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ train ‡πÇ‡∏°‡πÄ‡∏î‡∏•
- **‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö target ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î** ‡∏Ñ‡∏∑‡∏≠ bmi ‡πÅ‡∏•‡∏∞ s5 (‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå 0.59 ‡πÅ‡∏•‡∏∞ 0.57 ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö)
- ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå feature importance ‡∏Ç‡∏≠‡∏á Random Forest ‡∏Å‡πá‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ bmi ‡πÅ‡∏•‡∏∞ s5 ‡πÄ‡∏õ‡πá‡∏ô feature ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¥‡∏ó‡∏ò‡∏¥‡∏û‡∏•‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô
- ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß dataset ‡∏ô‡∏µ‡πâ‡∏°‡∏µ **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á** ‚Äì ‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ Linear Regression ‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö actual vs predicted ‡∏¢‡∏±‡∏á‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
---
### Notebook
### üëâ View the full code and analysis in Google Colab: https://colab.research.google.com/drive/1-2G1mkhT9AJHWs9Kz0MCbKWa3AoTbEN6?usp=sharing
---
### Possible Improvements (next iteration)
- Hyperparameter tuning  
- Add support vector machine (SVM) model for comparison  
- Implement K-fold cross validation  
- Build simple Streamlit UI for prediction
---
